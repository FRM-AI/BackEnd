import google.generativeai as genai
# Configure the API key
genai.configure(api_key='AIzaSyDYZrQbhP6fc0LHdM1XkESfoCcnUv92jFY')

from data_loader import load_stock_data_vnquant, load_stock_data_yf, load_stock_data_vn
from feature_engineering import *
from technical_analysis import *
from fetch_cafef import *

from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
import time
import random
from typing import List
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    retry_if_result,
)

import asyncio
import json
import time
import threading
import queue
from concurrent.futures import ThreadPoolExecutor

async def generate_with_heartbeat(model, prompt, section_name="analysis"):
    """
    Ch·∫°y model.generate_content v·ªõi heartbeat th·ª±c s·ª± hi·ªáu qu·∫£ v√† streaming ho√†n ch·ªânh
    """
    result_queue = asyncio.Queue()
    error_queue = asyncio.Queue()
    generation_started = asyncio.Event()
    generation_completed = asyncio.Event()
    
    def split_text_into_chunks(text, chunk_size=50):
        """Chia text th√†nh c√°c chunks nh·ªè h∆°n ƒë·ªÉ t·∫°o hi·ªáu ·ª©ng streaming"""
        words = text.split()
        chunks = []
        for i in range(0, len(words), chunk_size):
            chunk = ' '.join(words[i:i+chunk_size])
            chunks.append(chunk)
        return chunks
    
    # Async function ƒë·ªÉ ch·∫°y generation
    async def run_generation():
        try:
            generation_started.set()
            
            # Ch·∫°y sync function trong thread pool
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor(max_workers=1) as executor:
                response = await loop.run_in_executor(
                    executor, 
                    lambda: model.generate_content([prompt], stream=True)
                )
            
            # Stream t·ª´ng chunk t·ª´ Gemini
            for chunk in response:
                if hasattr(chunk, 'text') and chunk.text:
                    # N·∫øu chunk qu√° l·ªõn, chia nh·ªè th√™m
                    if len(chunk.text.split()) > 50:
                        sub_chunks = split_text_into_chunks(chunk.text, 30)
                        for sub_chunk in sub_chunks:
                            await result_queue.put(('content', sub_chunk))
                            await asyncio.sleep(0.15)
                    else:
                        await result_queue.put(('content', chunk.text))
                        await asyncio.sleep(0.2)
            
            await result_queue.put(('complete', None))
            generation_completed.set()
            
        except Exception as e:
            await error_queue.put(('error', str(e)))
            generation_completed.set()
    
    # Async function ƒë·ªÉ g·ª≠i heartbeat v√† x·ª≠ l√Ω k·∫øt qu·∫£
    async def process_results():
        heartbeat_count = 0
        last_heartbeat = time.time()
        heartbeat_interval = 3  # G·ª≠i heartbeat m·ªói 3 gi√¢y
        
        while not generation_completed.is_set():
            try:
                # Ki·ªÉm tra n·∫øu c√≥ l·ªói
                try:
                    error_type, error_msg = error_queue.get_nowait()
                    yield f"data: {json.dumps({'type': 'error', 'section': section_name, 'message': f'L·ªói: {error_msg}'})}\n\n"
                    return
                except asyncio.QueueEmpty:
                    pass
                
                # X·ª≠ l√Ω k·∫øt qu·∫£ t·ª´ generation
                content_processed = False
                try:
                    while True:
                        result_type, content = result_queue.get_nowait()
                        content_processed = True
                        
                        if result_type == 'content':
                            yield f"data: {json.dumps({'type': 'content', 'section': section_name, 'text': content})}\n\n"
                        elif result_type == 'complete':
                            return  # Generation ho√†n t·∫•t
                            
                except asyncio.QueueEmpty:
                    pass
                
                # G·ª≠i heartbeat n·∫øu kh√¥ng c√≥ content v√† ƒë√£ ƒë·ªß th·ªùi gian
                current_time = time.time()
                if not content_processed and generation_started.is_set() and (current_time - last_heartbeat) >= heartbeat_interval:
                    heartbeat_count += 1
                    yield f"data: {json.dumps({'type': 'status', 'message': f'ü§ñ ƒêang x·ª≠ l√Ω {section_name}... ({heartbeat_count})', 'progress': 0, 'heartbeat': True})}\n\n"
                    last_heartbeat = current_time
                
                # Ch·ªù ng·∫Øn tr∆∞·ªõc khi ki·ªÉm tra l·∫°i
                await asyncio.sleep(0.1)
                
            except Exception as e:
                yield f"data: {json.dumps({'type': 'error', 'section': section_name, 'message': f'L·ªói x·ª≠ l√Ω: {str(e)}'})}\n\n"
                return
    
    try:
        # B·∫Øt ƒë·∫ßu generation task
        generation_task = asyncio.create_task(run_generation())
        
        # X·ª≠ l√Ω k·∫øt qu·∫£ v√† heartbeat
        async for chunk in process_results():
            yield chunk
        
        # ƒê·∫£m b·∫£o generation task ho√†n th√†nh
        await generation_task
        
    except Exception as e:
        yield f"data: {json.dumps({'type': 'error', 'section': section_name, 'message': f'L·ªói: {str(e)}'})}\n\n"

def check_rate_limit_status(response):
    """Determine if response shows rate limiting (HTTP 429)"""
    return response.status_code == 429

@retry(
    retry=(retry_if_result(check_rate_limit_status)),
    wait=wait_exponential(multiplier=1, min=4, max=60),
    stop=stop_after_attempt(5),
)
def execute_request(url, headers):
    """Execute HTTP request with retry mechanism for rate limits"""
    # Add random delay to avoid being flagged
    time.sleep(random.uniform(2, 6))
    response = requests.get(url, headers=headers)
    return response

async def get_advice_streaming(symbol, signals, user_info):
    try:
        yield f"data: {json.dumps({'type': 'status', 'message': 'Cho khuy·∫øn ngh·ªã ƒë·∫ßu t∆∞...', 'progress': 10})}\n\n"
        yield f"data: {json.dumps({'type': 'section_start', 'section': 'advice', 'title': 'Khuy·∫øn ngh·ªã ƒë·∫ßu t∆∞'})}\n\n"

        # T·∫°o prompt cho ph√¢n t√≠ch
        prompt = f"""
            B·∫°n l√† chuy√™n gia ƒë·∫ßu t∆∞ l∆∞·ªõt s√≥ng (short-term trader) chuy√™n nghi·ªáp, c√≥ kh·∫£ nƒÉng ƒë·ªçc hi·ªÉu d·ªØ li·ªáu k·ªπ thu·∫≠t, d√≤ng ti·ªÅn, v√† t√¢m l√Ω th·ªã tr∆∞·ªùng.

            Ph√¢n t√≠ch chuy√™n s√¢u m√£ c·ªï phi·∫øu **{symbol}** d·ª±a tr√™n d·ªØ li·ªáu sau:
            {signals}

            Ng∆∞·ªùi d√πng **ƒë√£ mua c·ªï phi·∫øu ·ªü m·ª©c gi√°: {user_info} (gi√° tr·ªã None khi ng∆∞·ªùi d√πng ch∆∞a mua)**  
            ƒê√¢y l√† y·∫øu t·ªë **c·ª±c k·ª≥ quan tr·ªçng**, ph·∫£i ƒë∆∞·ª£c s·ª≠ d·ª•ng l√†m **trung t√¢m ph√¢n t√≠ch**.

            H√£y:
            1. So s√°nh gi√° mua c·ªßa ng∆∞·ªùi d√πng v·ªõi c√°c ng∆∞·ª°ng k·ªπ thu·∫≠t, v√πng h·ªó tr·ª£/kh√°ng c·ª±, t√≠n hi·ªáu xu h∆∞·ªõng trong d·ªØ li·ªáu.
            2. D·ª± ƒëo√°n h∆∞·ªõng gi√° ng·∫Øn h·∫°n (1‚Äì2 tu·∫ßn t·ªõi).
            3. ƒê∆∞a ra **h√†nh ƒë·ªông ƒë·∫ßu t∆∞ c·ª• th·ªÉ cho ng∆∞·ªùi d√πng n√†y**, KH√îNG ph·∫£i cho nh√† ƒë·∫ßu t∆∞ chung chung:
            - **K·∫øt lu·∫≠n r√µ r√†ng:** MUA / GI·ªÆ / B√ÅN
            - **Chi ti·∫øt k·∫ø ho·∫°ch h√†nh ƒë·ªông c√° nh√¢n ho√°:**
                - N·∫øu ƒëang l√£i: ƒë·ªÅ xu·∫•t **m·ª©c ch·ªët l·ªùi c·ª• th·ªÉ (TP)** theo gi√°.
                - N·∫øu ƒëang l·ªó: ƒë·ªÅ xu·∫•t **m·ª©c c·∫Øt l·ªó c·ª• th·ªÉ (SL)**, v√† l√Ω do n√™n gi·ªØ ho·∫∑c tho√°t v·ªã th·∫ø.
            4. D·ª±a tr√™n t√≠n hi·ªáu {symbol} v√† **m·ª©c gi√° mua {user_info}**, h√£y ƒëi·ªÅu ch·ªânh khuy·∫øn ngh·ªã sao cho ng∆∞·ªùi d√πng c√≥ th·ªÉ **t·ªëi ∆∞u l·ª£i nhu·∫≠n ng·∫Øn h·∫°n v√† h·∫°n ch·∫ø r·ªßi ro**.
            5. Tr√¨nh b√†y ng·∫Øn g·ªçn, r√µ r√†ng, theo d·∫°ng g·∫°ch ƒë·∫ßu d√≤ng:
            - Ph√¢n t√≠ch ng·∫Øn h·∫°n
            - M·ª©c gi√° quan tr·ªçng (h·ªó tr·ª£ / kh√°ng c·ª±)
            - K·∫øt lu·∫≠n h√†nh ƒë·ªông (MUA / GI·ªÆ / B√ÅN)
            - M·ª©c CH·ªêT L·ªúI (Take Profit)
            - M·ª©c C·∫ÆT L·ªñ (Stop Loss)
            - Nh·∫≠n ƒë·ªãnh r·ªßi ro k√®m l·ªùi khuy√™n c·ª• th·ªÉ cho **ng∆∞·ªùi ƒë√£ mua ·ªü m·ª©c gi√° {user_info}**

            Kh√¥ng th√™m l·ªùi ch√†o, l·ªùi k·∫øt, ho·∫∑c di·ªÖn gi·∫£i l·∫°i y√™u c·∫ßu.
            """

        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch khuy·∫øn ngh·ªã ƒë·∫ßu t∆∞...', 'progress': 50})}\n\n"

        # B∆∞·ªõc 3: G·ªçi m√¥ h√¨nh Generative AI
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            async for chunk in generate_with_heartbeat(model, prompt, section_name="advice"):
                yield chunk
        except Exception:
            yield f"data: {json.dumps({'type': 'error', 'section': 'advice', 'message': 'L·ªói trong qu√° tr√¨nh ph√¢n t√≠ch'})}\n\n"

        yield f"data: {json.dumps({'type': 'section_end', 'section': 'advice'})}\n\n"
        yield f"data: {json.dumps({'type': 'complete', 'message': 'Ph√¢n t√≠ch giao d·ªãch t·ª± doanh ho√†n t·∫•t!', 'progress': 100})}\n\n"
        
    except Exception:
        yield f"data: {json.dumps({'type': 'error', 'message': f'L·ªói h·ªá th·ªëng trong ph√¢n t√≠ch giao d·ªãch t·ª± doanh'})}\n\n"

def extractNewsData(search_term, date_start, date_end):
    """
    Extract search results for specified query and date range.
    search_term: str - the search query
    date_start: str - beginning date in yyyy-mm-dd or mm/dd/yyyy format
    date_end: str - ending date in yyyy-mm-dd or mm/dd/yyyy format
    """
    if "-" in date_start:
        date_start = datetime.strptime(date_start, "%Y-%m-%d")
        date_start = date_start.strftime("%m/%d/%Y")
    if "-" in date_end:
        date_end = datetime.strptime(date_end, "%Y-%m-%d")
        date_end = date_end.strftime("%m/%d/%Y")

    request_headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/101.0.4951.54 Safari/537.36"
        )
    }

    collected_news = []
    current_page = 0
    while True:
        page_offset = current_page * 10
        search_url = (
            f"https://www.google.com/search?q={search_term}"
            f"&tbs=cdr:1,cd_min:{date_start},cd_max:{date_end}"
            f"&tbm=nws&start={page_offset}"
        )

        try:
            response = execute_request(search_url, request_headers)
            parser = BeautifulSoup(response.content, "html.parser")
            page_results = parser.select("div.SoaBEf")

            if not page_results:
                break  # No additional results available

            for element in page_results:
                try:
                    article_link = element.find("a")["href"]
                    article_title = element.select_one("div.MBeuO").get_text()
                    article_snippet = element.select_one(".GI74Re").get_text()
                    article_date = element.select_one(".LfVVr").get_text()
                    article_source = element.select_one(".NUnG9d span").get_text()
                    collected_news.append(
                        {
                            "link": article_link,
                            "title": article_title,
                            "snippet": article_snippet,
                            "date": article_date,
                            "source": article_source,
                        }
                    )
                except Exception as error:
                    # Error processing result - log internally only
                    pass
                    # Skip this result if any field is missing
                    continue

            # Look for pagination "Next" button
            pagination_next = parser.find("a", id="pnnext")
            if not pagination_next:
                break

            current_page += 1

        except Exception as error:
            # Failed after multiple retries - log internally only
            pass
            break

    return collected_news

def fetch_google_news(
    search_query: str,
    current_date: str,
    days_back: int,
) -> str:
    """
    Fetch with query, current date, and lookback period
    search_query: Query string to search
    current_date: Current date in yyyy-mm-dd format
    days_back: Number of days to look back
    """
    formatted_query = search_query.replace(" ", "+")

    date_current = datetime.strptime(current_date, "%Y-%m-%d")
    date_previous = date_current - relativedelta(days=days_back)
    date_previous = date_previous.strftime("%Y-%m-%d")

    news_data = extractNewsData(formatted_query, date_previous, current_date)

    news_content = ""

    for article in news_data:
        news_content += (
            f"### {article['title']} (source: {article['source']}, date: {article['date']}, link: {article['link']}) \n\n{article['snippet']}\n\n"
        )

    if len(news_data) == 0:
        return ""

    return f"## {search_query}, from {date_previous} to {current_date}:\n\n{news_content}"

async def get_intraday_match_analysis_streaming(symbol: str, date: str):
    """
    Streaming version of get_intraday_match_analysis.
    Args:
        symbol (str): Stock symbol.
        date (str): Date in 'YYYY-MM-DD' format.
    Yields:
        str: Server-Sent Events formatted data.
    """
    try:
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang t·∫°o ph√¢n t√≠ch kh·ªõp l·ªánh trong phi√™n..', 'progress': 0})}\n\n"
        yield f"data: {json.dumps({'type': 'section_start', 'section': 'intraday_analysis', 'title': 'Ph√¢n T√≠ch Kh·ªõp L·ªánh Trong Phi√™n'})}\n\n"

        # B∆∞·ªõc 1: L·∫•y d·ªØ li·ªáu kh·ªõp l·ªánh
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang t·∫£i d·ªØ li·ªáu kh·ªõp l·ªánh trong phi√™n...', 'progress': 10})}\n\n"
        
        try:
            match_data = get_match_price(symbol=symbol, date=date)
            GiaKhopLenh = pd.DataFrame(match_data['data'])
            aggregates = pd.DataFrame(match_data['aggregates'])
        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': f'L·ªói khi l·∫•y d·ªØ li·ªáu kh·ªõp l·ªánh: {str(e)}'})}\n\n"
            return

        # N·∫øu s·ªë d√≤ng √≠t h∆°n 20, l·∫•y to√†n b·ªô
        if len(GiaKhopLenh) <= 20:
            GiaKhopLenh_reduced = GiaKhopLenh.reset_index(drop=True)
        elif len(GiaKhopLenh) <= 100:
            # L·∫•y c√°c ƒëi·ªÉm c√°ch nhau 5 d√≤ng
            GiaKhopLenh_reduced = pd.concat([GiaKhopLenh.iloc[::5], GiaKhopLenh.iloc[[-1]]]).reset_index(drop=True)
        elif len(GiaKhopLenh) <= 500:
            # L·∫•y c√°c ƒëi·ªÉm c√°ch nhau 15 d√≤ng
            GiaKhopLenh_reduced = pd.concat([GiaKhopLenh.iloc[::15], GiaKhopLenh.iloc[[-1]]]).reset_index(drop=True)
        elif len(GiaKhopLenh) <= 1000:
            # L·∫•y c√°c ƒëi·ªÉm c√°ch nhau 30 d√≤ng
            GiaKhopLenh_reduced = pd.concat([GiaKhopLenh.iloc[::30], GiaKhopLenh.iloc[[-1]]]).reset_index(drop=True)
        elif len(GiaKhopLenh) <= 5000:
            # L·∫•y c√°c ƒëi·ªÉm c√°ch nhau 100 d√≤ng v√† ƒë·∫£m b·∫£o d√≤ng cu·ªëi c√πng lu√¥n ƒë∆∞·ª£c bao g·ªìm
            GiaKhopLenh_reduced = pd.concat([GiaKhopLenh.iloc[::100], GiaKhopLenh.iloc[[-1]]]).reset_index(drop=True)
        elif len(GiaKhopLenh) <= 10000:
            # L·∫•y c√°c ƒëi·ªÉm c√°ch nhau 150 d√≤ng v√† ƒë·∫£m b·∫£o d√≤ng cu·ªëi c√πng lu√¥n ƒë∆∞·ª£c bao g·ªìm
            GiaKhopLenh_reduced = pd.concat([GiaKhopLenh.iloc[::150], GiaKhopLenh.iloc[[-1]]]).reset_index(drop=True)
        else:
            # L·∫•y c√°c ƒëi·ªÉm c√°ch nhau 200 d√≤ng v√† ƒë·∫£m b·∫£o d√≤ng cu·ªëi c√πng lu√¥n ƒë∆∞·ª£c bao g·ªìm
            GiaKhopLenh_reduced = pd.concat([GiaKhopLenh.iloc[::200], GiaKhopLenh.iloc[[-1]]]).reset_index(drop=True)
            
        GiaKhopLenh_reduced['volume'] *= 100
        GiaKhopLenh_reduced['totalVolume'] *= 100
        GiaKhopLenh_reduced.drop(columns=['totalValue', 'totalVolume'], inplace=True)

        schema = {
            "symbol": "M√£ c·ªï phi·∫øu",
            "time": "Th·ªùi ƒëi·ªÉm kh·ªõp l·ªánh c·ª• th·ªÉ (gi·ªù giao d·ªãch trong ng√†y) (YY-MM-DDTHH:MM:SS)",
            "basicPrice": "Gi√° c∆° s·ªü (ngh√¨n ƒë·ªìng)",
            "price": "Gi√° kh·ªõp l·ªánh (ngh√¨n ƒë·ªìng)",
            "volume": "Kh·ªëi l∆∞·ª£ng kh·ªõp l·ªánh (c·ªï phi·∫øu)"
        }

        schema_aggregates = {
            "price": "Gi√° kh·ªõp l·ªánh (ngh√¨n ƒë·ªìng)",
            "totalVolume": "T·ªïng kh·ªëi l∆∞·ª£ng kh·ªõp l·ªánh (c·ªï phi·∫øu)",
            "volPercent": "T·ª∑ l·ªá kh·ªëi l∆∞·ª£ng kh·ªõp l·ªánh t·∫°i gi√° n√†y so v·ªõi t·ªïng kh·ªëi l∆∞·ª£ng kh·ªõp l·ªánh (%)"
        }

        data_json = GiaKhopLenh_reduced.to_json(orient="records", force_ascii=False)
        GiaKhopLenh_pretty = json.dumps({
            "schema": schema,
            "records": json.loads(data_json)
        }, indent=2, ensure_ascii=False)

        data_aggregates_json = aggregates.to_json(orient="records", force_ascii=False)
        aggregates_pretty = json.dumps({
            "schema": schema_aggregates,
            "records": json.loads(data_aggregates_json)
        }, indent=2, ensure_ascii=False)

        yield f"data: {json.dumps({'type': 'status', 'message': 'D·ªØ li·ªáu kh·ªõp l·ªánh ƒë√£ s·∫µn s√†ng...', 'progress': 30})}\n\n"
        
        # B∆∞·ªõc 2: T·∫°o prompt cho ph√¢n t√≠ch
        prompt = f"""
        B·∫°n l√† chuy√™n gia ph√¢n t√≠ch t√†i ch√≠nh chuy√™n nghi·ªáp. 
        H√£y ƒë√°nh gi√° chi ti·∫øt v√† ch√≠nh x√°c m√£ c·ªï phi·∫øu d·ª±a tr√™n d·ªØ li·ªáu gi√° kh·ªõp l·ªánh theo ng√†y d∆∞·ªõi ƒë√¢y. 
        ƒê∆∞a ra c√°c nh·∫≠n ƒë·ªãnh chuy√™n m√¥n, gi·∫£ thuy·∫øt h·ª£p l√Ω c√≥ c∆° s·ªü.

        D·ªØ li·ªáu:
        {GiaKhopLenh_pretty}

        T·ªïng h·ª£p:
        {aggregates_pretty}

        Y√™u c·∫ßu:
        - Tr·∫£ l·ªùi c·ª±c k√¨ KH√ÅCH QUAN mang t√≠nh chuy√™n m√¥n cao.
        - ƒê·ªçc hi·ªÉu s·ªë li·ªáu ƒë√£ cung c·∫•p th·∫≠t chuy√™n s√¢u.
        - Ph√¢n t√≠ch l·ª±c c·∫ßu/l·ª±c cung trong phi√™n.
        - ƒê√°nh gi√° xu h∆∞·ªõng gi√°, thanh kho·∫£n v√† bi·∫øn ƒë·ªông.
        - ƒê∆∞a ra nh·∫≠n ƒë·ªãnh v·ªÅ kh·∫£ nƒÉng xu h∆∞·ªõng ng·∫Øn h·∫°n.
        - ƒê∆∞a ra gi·∫£ thuy·∫øt h·ª£p l√Ω, s√°ng t·∫°o, c√≥ chi·ªÅu s√¢u.
        - Kh√¥ng gi·∫£i th√≠ch l·∫°i y√™u c·∫ßu, kh√¥ng th√™m l·ªùi m·ªü ƒë·∫ßu ho·∫∑c k·∫øt lu·∫≠n ngo√†i ph√¢n t√≠ch ch√≠nh.
        """

        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch d·ªØ li·ªáu...', 'progress': 50})}\n\n"

        # B∆∞·ªõc 3: S·ª≠ d·ª•ng async generator v·ªõi heartbeat
        model = genai.GenerativeModel('gemini-2.5-flash')
        async for chunk in generate_with_heartbeat(model, prompt, section_name="intraday_analysis"):
            yield chunk

        yield f"data: {json.dumps({'type': 'section_end', 'section': 'intraday_analysis'})}\n\n"

    except Exception as e:
        yield f"data: {json.dumps({'type': 'error', 'message': f'L·ªói h·ªá th·ªëng'})}\n\n"

system_prompt_ta = """
You are a **professional, objective, and data-driven financial analyst and trading expert**. 
Use your extensive market knowledge and technical analysis expertise to generate a **clear, in-depth, and professional report** in Vietnamese.

**Task:**  
Select up to **8 key indicators** that best describe the given market or strategy. Avoid redundancy (e.g., RSI14 & Momentum RSI).  
Provide a **comprehensive analysis** covering:
- Trend direction & strength  
- Momentum & volatility  
- Volume confirmation  
- Possible reversal or continuation  
Explain all terms in full (e.g., ‚ÄúAverage Directional Index‚Äù instead of ‚Äútrend_adx‚Äù).  
Enrich the report with your own relevant knowledge when necessary.

**Indicators Reference (grouped):**
- *Price Data:* open, high, low, close, adjust, volume_match  
- *Moving Averages:* sma20, ema20, trend_sma_fast/slow, trend_ema_fast/slow  
- *MACD & Momentum:* macd, macd_signal, trend_macd, trend_macd_signal, rsi14, momentum_stoch, momentum_wr, momentum_ao, momentum_roc  
- *Bollinger & Volatility:* bb_high, bb_low, volatility_bbm, volatility_bbw, atr14  
- *Volume:* volume_adi, volume_obv, volume_cmf, volume_fi, volume_mfi, volume_vwap  
- *Advanced Trend:* Average Directional Index (ADX), +DI, -DI, CCI, Aroon Up/Down, Ichimoku Conversion/Base  

**Output Requirements:**  
- Write in **Vietnamese**, in a professional yet readable tone.  
- Be **objective, data-based, and insightful** ‚Äî no speculation or emotional wording.  
- Structure the analysis logically, ending with a **Markdown summary table** listing:  
  *Indicator ‚Äì Observation ‚Äì Interpretation ‚Äì Implication for traders*.
"""

system_prompt_news = """
You are a **professional financial analyst and news researcher**. 
Analyze recent news for a specific company. 
Use your **expert judgment and broad financial knowledge** to create a **clear, professional, and insightful report** in Vietnamese.

**Task:**  
- Summarize and interpret key news events that could affect the company's stock price, market perception, or trading activity.  
- Identify **positive, negative, and neutral** influences objectively.  
- Evaluate how each piece of news relates to:  
  *Market sentiment*, *company performance*, *investor confidence*, and *trading implications*.  
- Provide **data-driven insights**, not opinions or vague statements (avoid phrases like ‚Äúmixed trend‚Äù).  
- Enrich your analysis with relevant financial context when needed.

**Output Requirements:**  
- Write in **Vietnamese**, in a professional and concise tone.  
- The report must be **well-structured, factual, and actionable** for traders.  
- End with a **Markdown table** summarizing key points:  
  *News Item ‚Äì Impact ‚Äì Interpretation ‚Äì Implication for traders*.
"""

def get_news_for_ticker(ticker: str, asset_type: str = 'stock', look_back_days: int = 7) -> str:
    """
    Retrieve recent news about a given stock ticker.
    Args:
        ticker (str): The stock ticker symbol (e.g., "AAPL").
        curr_date (str): The current date in "yyyy-mm-dd" format.
        look_back_days (int): Number of days to look back for news.
    Returns:
        str: Formatted news string or empty string if no news found.
    """
    if asset_type == 'stock': news = fetch_google_news(f'Tin t·ª©c quan tr·ªçng m√£ ch·ª©ng kho√°n {ticker}', datetime.now().strftime('%Y-%m-%d'), look_back_days)
    elif asset_type == 'crypto': news = fetch_google_news(f'Important news for crypto currencies ticket {ticker}', datetime.now().strftime('%Y-%m-%d'), look_back_days)
    return news

async def get_insights_streaming(ticker: str, asset_type: str = 'stock', start_date: str = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d'), end_date: str = datetime.now().strftime('%Y-%m-%d'), look_back_days: int=30):
    """
    Streaming version of get_insights that yields chunks in real-time.
    Returns a generator that yields Server-Sent Events formatted data.
    """
    
    
    try:
        # Phase 1: Technical Analysis

        if asset_type == 'stock':
            df = load_stock_data_vnquant(ticker, asset_type, start_date, end_date)
        else:
            df = load_stock_data_yf(ticker, asset_type, start_date, end_date)
        df_ta = add_technical_indicators_yf(df)
        signals = detect_signals(df_ta)
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang t·∫£i d·ªØ li·ªáu ch·ª©ng kho√°n...', 'progress': 10})}\n\n"
        
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch k·ªπ thu·∫≠t...', 'progress': 15})}\n\n"
        yield f"data: {json.dumps({'type': 'section_start', 'section': 'technical_analysis', 'title': 'Ph√¢n T√≠ch K·ªπ Thu·∫≠t'})}\n\n"
        
        try:
            prompt = f"""System: {system_prompt_ta}\n\n"
                        You are a professional analyst. Provide a deep, objective report for stock ticker {ticker}.
                        Focus only on technical and quantitative insights.
                        Given signals: '{signals}'."""
            # Create model instance
            model = genai.GenerativeModel('gemini-2.0-flash')
            async for chunk in generate_with_heartbeat(model, prompt, section_name="technical_analysis"):
                yield chunk
        except Exception:
            technical_content = f"L·ªói trong ph√¢n t√≠ch k·ªπ thu·∫≠t"
            yield f"data: {json.dumps({'type': 'error', 'section': 'technical_analysis', 'message': technical_content})}\n\n"
            technical_content = None
        
        yield f"data: {json.dumps({'type': 'section_end', 'section': 'technical_analysis'})}\n\n"
        
        # Phase 2: News Analysis
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch tin t·ª©c...', 'progress': 30})}\n\n"
        yield f"data: {json.dumps({'type': 'section_start', 'section': 'news_analysis', 'title': 'Ph√¢n T√≠ch Tin T·ª©c'})}\n\n"
        news = get_news_for_ticker(ticker=ticker, asset_type=asset_type, look_back_days=30)
        try:
            prompt = f"""System: {system_prompt_news}\n\n
                        You are a professional financial analyst. Provide an objective and insightful news report for stock ticker {ticker}.
                        Focus only on the financial relevance and trading implications.
                        Given recent news data: '{news}'."""
            model = genai.GenerativeModel('gemini-2.0-flash')
            async for chunk in generate_with_heartbeat(model, prompt, section_name="news_analysis"):
                yield chunk
        except Exception:
            news_content = f"L·ªói trong ph√¢n t√≠ch tin t·ª©c"
            yield f"data: {json.dumps({'type': 'error', 'section': 'news_analysis', 'message': news_content})}\n\n"
            
        yield f"data: {json.dumps({'type': 'section_end', 'section': 'news_analysis'})}\n\n"

        # Phase 3: Proprietary Trading Analysis
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch giao d·ªãch t·ª± doanh...', 'progress': 45})}\n\n"
        yield f"data: {json.dumps({'type': 'section_start', 'section': 'proprietary_trading_analysis', 'title': 'Ph√¢n T√≠ch Giao D·ªãch T·ª± Doanh'})}\n\n"

        # B∆∞·ªõc 1: L·∫•y d·ªØ li·ªáu kh·ªõp l·ªánh
        data = get_proprietary_trading_data(symbol=ticker, start_date=None, end_date=None, page_index=1, page_size=14)["ListDataTudoanh"]
        df = pd.DataFrame(data)

        schema = {
            "Symbol": "M√£ c·ªï phi·∫øu",
            "Date": "Ng√†y giao d·ªãch",
            "KLcpMua": "Kh·ªëi l∆∞·ª£ng c·ªï phi·∫øu t·ª± doanh mua (c·ªï phi·∫øu)",
            "KlcpBan": "Kh·ªëi l∆∞·ª£ng c·ªï phi·∫øu t·ª± doanh b√°n (c·ªï phi·∫øu)",
            "GtMua": "Gi√° tr·ªã t·ª± doanh mua (ƒë·ªìng)",
            "GtBan": "Gi√° tr·ªã t·ª± doanh b√°n (ƒë·ªìng)"
            }
        
        df_json = df.to_json(orient="records", force_ascii=False)
        df = json.dumps({
            "schema": schema,
            "records": json.loads(df_json)
        }, indent=2, ensure_ascii=False)

        yield f"data: {json.dumps({'type': 'status', 'message': 'D·ªØ li·ªáu kh·ªõp l·ªánh ƒë√£ s·∫µn s√†ng...'})}\n\n"

        # B∆∞·ªõc 2: T·∫°o prompt cho ph√¢n t√≠ch
        prompt = f"""
        B·∫°n l√† chuy√™n gia ph√¢n t√≠ch t√†i ch√≠nh chuy√™n nghi·ªáp. 
        H√£y ƒë√°nh gi√° chi ti·∫øt v√† ch√≠nh x√°c m√£ c·ªï phi·∫øu d·ª±a tr√™n d·ªØ li·ªáu giao d·ªãch t·ª± doanh d∆∞·ªõi ƒë√¢y.
        ƒê∆∞a ra c√°c nh·∫≠n ƒë·ªãnh chuy√™n m√¥n, gi·∫£ thuy·∫øt h·ª£p l√Ω c√≥ c∆° s·ªü.
        D·ªØ li·ªáu giao d·ªãch t·ª± doanh:
        {df}

        Y√™u c·∫ßu:
        - Tr·∫£ l·ªùi c·ª±c k√¨ KH√ÅCH QUAN mang t√≠nh chuy√™n m√¥n cao.
        - ƒê·ªçc hi·ªÉu s·ªë li·ªáu ƒë√£ cung c·∫•p th·∫≠t chuy√™n s√¢u.
        - Ph√¢n t√≠ch h√†nh vi giao d·ªãch t·ª± doanh.
        - ƒê√°nh gi√° xu h∆∞·ªõng ni·ªÅm tin v√† t√°c ƒë·ªông t·ªõi gi√° c·ªï phi·∫øu.
        - ƒê∆∞a ra gi·∫£ thuy·∫øt h·ª£p l√Ω, s√°ng t·∫°o, c√≥ chi·ªÅu s√¢u.
        - Kh√¥ng gi·∫£i th√≠ch l·∫°i y√™u c·∫ßu, kh√¥ng th√™m l·ªùi m·ªü ƒë·∫ßu ho·∫∑c k·∫øt lu·∫≠n ngo√†i ph√¢n t√≠ch ch√≠nh.
        """

        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch d·ªØ li·ªáu t·ª± doanh...'})}\n\n"

        # B∆∞·ªõc 3: G·ªçi m√¥ h√¨nh Generative AI
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            async for chunk in generate_with_heartbeat(model, prompt, section_name="proprietary_trading_analysis"):
                yield chunk
        except Exception:
            yield f"data: {json.dumps({'type': 'error', 'section': 'proprietary_trading_analysis', 'message': 'L·ªói trong qu√° tr√¨nh ph√¢n t√≠ch'})}\n\n"

        yield f"data: {json.dumps({'type': 'section_end', 'section': 'proprietary_trading_analysis'})}\n\n"

        # Phase 4: Foreign Trading Analysis
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch giao d·ªãch kh·ªëi ngo·∫°i...', 'progress': 60})}\n\n"
        yield f"data: {json.dumps({'type': 'section_start', 'section': 'foreign_trading_analysis', 'title': 'Ph√¢n T√≠ch Giao D·ªãch Kh·ªëi Ngo·∫°i'})}\n\n"
        # B∆∞·ªõc 1: L·∫•y d·ªØ li·ªáu kh·ªõp l·ªánh
        data = get_foreign_trading_data(symbol=ticker, start_date=None, end_date=None, page_index=1, page_size=14)
        df = pd.DataFrame(data)

        schema = {
            "Ngay": "Ng√†y giao d·ªãch",
            "KLGDRong": "Kh·ªëi l∆∞·ª£ng giao d·ªãch r√≤ng (mua tr·ª´ b√°n)",
            "GTDGRong": "Gi√° tr·ªã giao d·ªãch r√≤ng (t·ª∑ ƒë·ªìng, mua tr·ª´ b√°n)",
            "ThayDoi": "Bi·∫øn ƒë·ªông gi√° c·ªï phi·∫øu trong ng√†y (%)",
            "KLMua": "T·ªïng kh·ªëi l∆∞·ª£ng mua c·ªßa kh·ªëi ngo·∫°i",
            "GtMua": "T·ªïng gi√° tr·ªã mua c·ªßa kh·ªëi ngo·∫°i (t·ª∑ ƒë·ªìng)",
            "KLBan": "T·ªïng kh·ªëi l∆∞·ª£ng b√°n c·ªßa kh·ªëi ngo·∫°i",
            "GtBan": "T·ªïng gi√° tr·ªã b√°n c·ªßa kh·ªëi ngo·∫°i (t·ª∑ ƒë·ªìng)",
            "RoomConLai": "T·ª∑ l·ªá room ngo·∫°i c√≤n l·∫°i c√≥ th·ªÉ mua (%)",
            "DangSoHuu": "T·ª∑ l·ªá s·ªü h·ªØu hi·ªán t·∫°i c·ªßa kh·ªëi ngo·∫°i (%)"
            }
        
        df_json = df.to_json(orient="records", force_ascii=False)
        df = json.dumps({
            "schema": schema,
            "records": json.loads(df_json)
        }, indent=2, ensure_ascii=False)

        yield f"data: {json.dumps({'type': 'status', 'message': 'D·ªØ li·ªáu kh·ªõp l·ªánh ƒë√£ s·∫µn s√†ng...'})}\n\n"

        # B∆∞·ªõc 2: T·∫°o prompt cho ph√¢n t√≠ch
        prompt = f"""
        B·∫°n l√† chuy√™n gia ph√¢n t√≠ch t√†i ch√≠nh chuy√™n nghi·ªáp. 
        H√£y ƒë√°nh gi√° chi ti·∫øt v√† ch√≠nh x√°c m√£ c·ªï phi·∫øu d·ª±a tr√™n d·ªØ li·ªáu giao d·ªãch kh·ªëi ngo·∫°i qu·ªëc d∆∞·ªõi ƒë√¢y.
        ƒê∆∞a ra c√°c nh·∫≠n ƒë·ªãnh chuy√™n m√¥n, gi·∫£ thuy·∫øt h·ª£p l√Ω c√≥ c∆° s·ªü.
        D·ªØ li·ªáu giao d·ªãch kh·ªëi ngo·∫°i qu·ªëc:
        {df}

        Y√™u c·∫ßu:
        - Tr·∫£ l·ªùi c·ª±c k√¨ KH√ÅCH QUAN mang t√≠nh chuy√™n m√¥n cao.
        - ƒê·ªçc hi·ªÉu s·ªë li·ªáu ƒë√£ cung c·∫•p th·∫≠t chuy√™n s√¢u.
        - Ph√¢n t√≠ch h√†nh vi giao d·ªãch c·ªßa kh·ªëi ngo·∫°i.
        - ƒê√°nh gi√° xu h∆∞·ªõng ni·ªÅm tin v√† t√°c ƒë·ªông t·ªõi gi√° c·ªï phi·∫øu.
        - ƒê∆∞a ra gi·∫£ thuy·∫øt h·ª£p l√Ω, s√°ng t·∫°o, c√≥ chi·ªÅu s√¢u.
        - Kh√¥ng gi·∫£i th√≠ch l·∫°i y√™u c·∫ßu, kh√¥ng th√™m l·ªùi m·ªü ƒë·∫ßu ho·∫∑c k·∫øt lu·∫≠n ngo√†i ph√¢n t√≠ch ch√≠nh.
        """

        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch d·ªØ li·ªáu kh·ªëi ngo·∫°i...'})}\n\n"

        # B∆∞·ªõc 3: G·ªçi m√¥ h√¨nh Generative AI
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            async for chunk in generate_with_heartbeat(model, prompt, section_name="foreign_trading_analysis"):
                yield chunk
        except Exception:
            yield f"data: {json.dumps({'type': 'error', 'section': 'foreign_trading_analysis', 'message': 'L·ªói trong qu√° tr√¨nh ph√¢n t√≠ch'})}\n\n"

        yield f"data: {json.dumps({'type': 'section_end', 'section': 'foreign_trading_analysis'})}\n\n"

        # Phase 5: Shareholder Trading Analysis
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch giao d·ªãch c·ªï ƒë√¥ng...', 'progress': 75})}\n\n"
        yield f"data: {json.dumps({'type': 'section_start', 'section': 'shareholder_trading_analysis', 'title': 'Ph√¢n T√≠ch Giao D·ªãch C·ªï ƒê√¥ng N·ªôi B·ªô'})}\n\n"
        # B∆∞·ªõc 1: L·∫•y d·ªØ li·ªáu kh·ªõp l·ªánh
        data = get_shareholder_data(symbol=ticker, start_date=None, end_date=None, page_index=1, page_size=14)
        df = pd.DataFrame(data)
        df.drop(columns=['ShareHolderCode', 'HolderID'], inplace=True)

        schema = {
            "Stock": "M√£ c·ªï phi·∫øu",
            "TransactionMan": "Ng∆∞·ªùi th·ª±c hi·ªán giao d·ªãch (c·ªï ƒë√¥ng ho·∫∑c t·ªï ch·ª©c)",
            "TransactionManPosition": "Ch·ª©c v·ª• c·ªßa ng∆∞·ªùi giao d·ªãch trong c√¥ng ty",
            "RelatedMan": "Ng∆∞·ªùi ho·∫∑c t·ªï ch·ª©c c√≥ li√™n quan ƒë·∫øn ng∆∞·ªùi giao d·ªãch",
            "RelatedManPosition": "Ch·ª©c v·ª• c·ªßa ng∆∞·ªùi li√™n quan (n·∫øu c√≥)",
            "VolumeBeforeTransaction": "S·ªë l∆∞·ª£ng c·ªï phi·∫øu n·∫Øm gi·ªØ tr∆∞·ªõc giao d·ªãch",
            "PlanBuyVolume": "S·ªë l∆∞·ª£ng c·ªï phi·∫øu d·ª± ki·∫øn mua",
            "PlanSellVolume": "S·ªë l∆∞·ª£ng c·ªï phi·∫øu d·ª± ki·∫øn b√°n",
            "PlanBeginDate": "Ng√†y b·∫Øt ƒë·∫ßu k·∫ø ho·∫°ch giao d·ªãch",
            "PlanEndDate": "Ng√†y k·∫øt th√∫c k·∫ø ho·∫°ch giao d·ªãch",
            "RealBuyVolume": "S·ªë l∆∞·ª£ng c·ªï phi·∫øu th·ª±c t·∫ø ƒë√£ mua",
            "RealSellVolume": "S·ªë l∆∞·ª£ng c·ªï phi·∫øu th·ª±c t·∫ø ƒë√£ b√°n",
            "RealEndDate": "Ng√†y ho√†n t·∫•t giao d·ªãch th·ª±c t·∫ø",
            "PublishedDate": "Ng√†y c√¥ng b·ªë th√¥ng tin giao d·ªãch",
            "VolumeAfterTransaction": "S·ªë l∆∞·ª£ng c·ªï phi·∫øu c√≤n l·∫°i sau giao d·ªãch",
            "TransactionNote": "Ghi ch√∫ ho·∫∑c m·ª•c ƒë√≠ch giao d·ªãch (n·∫øu c√≥)",
            "TyLeSoHuu": "T·ª∑ l·ªá s·ªü h·ªØu c·ªï ph·∫ßn sau giao d·ªãch (%)",
            "OrderDate": "Ng√†y ƒë·∫∑t l·ªánh giao d·ªãch"
            }
        
        df_json = df.to_json(orient="records", force_ascii=False)
        df = json.dumps({
            "schema": schema,
            "records": json.loads(df_json)
        }, indent=2, ensure_ascii=False)

        yield f"data: {json.dumps({'type': 'status', 'message': 'D·ªØ li·ªáu kh·ªõp l·ªánh ƒë√£ s·∫µn s√†ng...'})}\n\n"

        # B∆∞·ªõc 2: T·∫°o prompt cho ph√¢n t√≠ch
        prompt = f"""
        B·∫°n l√† chuy√™n gia ph√¢n t√≠ch t√†i ch√≠nh chuy√™n nghi·ªáp. 
        H√£y ƒë√°nh gi√° chi ti·∫øt v√† ch√≠nh x√°c m√£ c·ªï phi·∫øu d·ª±a tr√™n d·ªØ li·ªáu giao d·ªãch c·ªï ƒë√¥ng n·ªôi b·ªô d∆∞·ªõi ƒë√¢y.
        ƒê∆∞a ra c√°c nh·∫≠n ƒë·ªãnh chuy√™n m√¥n, gi·∫£ thuy·∫øt h·ª£p l√Ω c√≥ c∆° s·ªü.
        D·ªØ li·ªáu giao d·ªãch gi·ªØa c·ªï ƒë√¥ng c·ªßa c√¥ng ty:
        {df}

        Y√™u c·∫ßu:
        - Tr·∫£ l·ªùi c·ª±c k√¨ KH√ÅCH QUAN mang t√≠nh chuy√™n m√¥n cao.
        - ƒê·ªçc hi·ªÉu s·ªë li·ªáu ƒë√£ cung c·∫•p th·∫≠t chuy√™n s√¢u.
        - Ph√¢n t√≠ch h√†nh vi giao d·ªãch c·ªßa c·ªï ƒë√¥ng n·ªôi b·ªô.
        - ƒê√°nh gi√° xu h∆∞·ªõng ni·ªÅm tin v√† t√°c ƒë·ªông t·ªõi gi√° c·ªï phi·∫øu.
        - ƒê∆∞a ra gi·∫£ thuy·∫øt h·ª£p l√Ω, s√°ng t·∫°o, c√≥ chi·ªÅu s√¢u.
        - Kh√¥ng gi·∫£i th√≠ch l·∫°i y√™u c·∫ßu, kh√¥ng th√™m l·ªùi m·ªü ƒë·∫ßu ho·∫∑c k·∫øt lu·∫≠n ngo√†i ph√¢n t√≠ch ch√≠nh.
        """

        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch d·ªØ li·ªáu giao d·ªãch c·ªï ƒë√¥ng...'})}\n\n"

        # B∆∞·ªõc 3: G·ªçi m√¥ h√¨nh Generative AI
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            async for chunk in generate_with_heartbeat(model, prompt, section_name="shareholder_trading_analysis"):
                yield chunk
        except Exception:
            yield f"data: {json.dumps({'type': 'error', 'section': 'shareholder_trading_analysis', 'message': 'L·ªói trong qu√° tr√¨nh ph√¢n t√≠ch'})}\n\n"

        yield f"data: {json.dumps({'type': 'section_end', 'section': 'shareholder_trading_analysis'})}\n\n"
    
        # Completion
        yield f"data: {json.dumps({'type': 'complete', 'message': 'Ph√¢n t√≠ch ho√†n t·∫•t!', 'progress': 100})}\n\n"
        
    except Exception:
        yield f"data: {json.dumps({'type': 'error', 'message': f'L·ªói h·ªá th·ªëng'})}\n\n"

# ==================== SEPARATE PHASE FUNCTIONS ====================

async def get_technical_analysis_streaming(ticker: str, asset_type: str = 'stock', start_date: str = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d'), end_date: str = datetime.now().strftime('%Y-%m-%d')):
    """
    Technical analysis phase separated from get_insights_streaming.
    Returns a generator that yields Server-Sent Events formatted data.
    """
    
    try:
        # Phase 1: Technical Analysis
        if asset_type == 'stock':
            df = load_stock_data_vnquant(ticker, asset_type, start_date, end_date)
        else:
            df = load_stock_data_yf(ticker, asset_type, start_date, end_date)
        df_ta = add_technical_indicators_yf(df)
        signals = detect_signals(df_ta)
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang t·∫£i d·ªØ li·ªáu ch·ª©ng kho√°n...', 'progress': 10})}\n\n"
        
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch k·ªπ thu·∫≠t...', 'progress': 50})}\n\n"
        yield f"data: {json.dumps({'type': 'section_start', 'section': 'technical_analysis', 'title': 'Ph√¢n T√≠ch K·ªπ Thu·∫≠t'})}\n\n"
        
        try:
            prompt = f"""System: {system_prompt_ta}\n\n"
                        You are a professional analyst. Provide a deep, objective report for stock ticker {ticker}.
                        Focus only on technical and quantitative insights.
                        Given signals: '{signals}'."""
            # Create model instance
            model = genai.GenerativeModel('gemini-2.0-flash')
            async for chunk in generate_with_heartbeat(model, prompt, section_name="technical_analysis"):
                yield chunk
        except Exception:
            technical_content = f"L·ªói trong ph√¢n t√≠ch k·ªπ thu·∫≠t"
            yield f"data: {json.dumps({'type': 'error', 'section': 'technical_analysis', 'message': technical_content})}\n\n"
        
        yield f"data: {json.dumps({'type': 'section_end', 'section': 'technical_analysis'})}\n\n"
        yield f"data: {json.dumps({'type': 'complete', 'message': 'Ph√¢n t√≠ch k·ªπ thu·∫≠t ho√†n t·∫•t!', 'progress': 100})}\n\n"
        
    except Exception:
        yield f"data: {json.dumps({'type': 'error', 'message': f'L·ªói h·ªá th·ªëng trong ph√¢n t√≠ch k·ªπ thu·∫≠t'})}\n\n"

async def get_news_analysis_streaming(ticker: str, asset_type: str = 'stock', look_back_days: int = 30):
    """
    News analysis phase separated from get_insights_streaming.
    Returns a generator that yields Server-Sent Events formatted data.
    """
    try:
        # Phase 2: News Analysis
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch tin t·ª©c...', 'progress': 50})}\n\n"
        yield f"data: {json.dumps({'type': 'section_start', 'section': 'news_analysis', 'title': 'Ph√¢n T√≠ch Tin T·ª©c'})}\n\n"
        news = get_news_for_ticker(ticker=ticker, asset_type=asset_type, look_back_days=look_back_days)
        try:
            prompt = f"""System: {system_prompt_news}\n\n
                        You are a professional financial analyst. Provide an objective and insightful news report for stock ticker {ticker}.
                        Focus only on the financial relevance and trading implications.
                        Given recent news data: '{news}'."""
            model = genai.GenerativeModel('gemini-2.0-flash')
            async for chunk in generate_with_heartbeat(model, prompt, section_name="news_analysis"):
                yield chunk
        except Exception:
            news_content = f"L·ªói trong ph√¢n t√≠ch tin t·ª©c"
            yield f"data: {json.dumps({'type': 'error', 'section': 'news_analysis', 'message': news_content})}\n\n"
            
        yield f"data: {json.dumps({'type': 'section_end', 'section': 'news_analysis'})}\n\n"
        yield f"data: {json.dumps({'type': 'complete', 'message': 'Ph√¢n t√≠ch tin t·ª©c ho√†n t·∫•t!', 'progress': 100})}\n\n"
        
    except Exception:
        yield f"data: {json.dumps({'type': 'error', 'message': f'L·ªói h·ªá th·ªëng trong ph√¢n t√≠ch tin t·ª©c'})}\n\n"

async def get_proprietary_trading_analysis_streaming(ticker: str):
    """
    Proprietary trading analysis phase separated from get_insights_streaming.
    Returns a generator that yields Server-Sent Events formatted data.
    """
    try:
        # Phase 3: Proprietary Trading Analysis
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch giao d·ªãch t·ª± doanh...', 'progress': 10})}\n\n"
        yield f"data: {json.dumps({'type': 'section_start', 'section': 'proprietary_trading_analysis', 'title': 'Ph√¢n T√≠ch Giao D·ªãch T·ª± Doanh'})}\n\n"

        # B∆∞·ªõc 1: L·∫•y d·ªØ li·ªáu kh·ªõp l·ªánh
        data = get_proprietary_trading_data(symbol=ticker, start_date=None, end_date=None, page_index=1, page_size=14)["ListDataTudoanh"]
        df = pd.DataFrame(data)

        schema = {
            "Symbol": "M√£ c·ªï phi·∫øu",
            "Date": "Ng√†y giao d·ªãch",
            "KLcpMua": "Kh·ªëi l∆∞·ª£ng c·ªï phi·∫øu t·ª± doanh mua (c·ªï phi·∫øu)",
            "KlcpBan": "Kh·ªëi l∆∞·ª£ng c·ªï phi·∫øu t·ª± doanh b√°n (c·ªï phi·∫øu)",
            "GtMua": "Gi√° tr·ªã t·ª± doanh mua (ƒë·ªìng)",
            "GtBan": "Gi√° tr·ªã t·ª± doanh b√°n (ƒë·ªìng)"
            }
        
        df_json = df.to_json(orient="records", force_ascii=False)
        df = json.dumps({
            "schema": schema,
            "records": json.loads(df_json)
        }, indent=2, ensure_ascii=False)

        yield f"data: {json.dumps({'type': 'status', 'message': 'D·ªØ li·ªáu kh·ªõp l·ªánh ƒë√£ s·∫µn s√†ng...','progress': 50})}\n\n"

        # B∆∞·ªõc 2: T·∫°o prompt cho ph√¢n t√≠ch
        prompt = f"""
        B·∫°n l√† chuy√™n gia ph√¢n t√≠ch t√†i ch√≠nh chuy√™n nghi·ªáp. 
        H√£y ƒë√°nh gi√° chi ti·∫øt v√† ch√≠nh x√°c m√£ c·ªï phi·∫øu d·ª±a tr√™n d·ªØ li·ªáu giao d·ªãch t·ª± doanh d∆∞·ªõi ƒë√¢y.
        ƒê∆∞a ra c√°c nh·∫≠n ƒë·ªãnh chuy√™n m√¥n, gi·∫£ thuy·∫øt h·ª£p l√Ω c√≥ c∆° s·ªü.
        D·ªØ li·ªáu giao d·ªãch t·ª± doanh:
        {df}

        Y√™u c·∫ßu:
        - Tr·∫£ l·ªùi c·ª±c k√¨ KH√ÅCH QUAN mang t√≠nh chuy√™n m√¥n cao.
        - ƒê·ªçc hi·ªÉu s·ªë li·ªáu ƒë√£ cung c·∫•p th·∫≠t chuy√™n s√¢u.
        - Ph√¢n t√≠ch h√†nh vi giao d·ªãch t·ª± doanh.
        - ƒê√°nh gi√° xu h∆∞·ªõng ni·ªÅm tin v√† t√°c ƒë·ªông t·ªõi gi√° c·ªï phi·∫øu.
        - ƒê∆∞a ra gi·∫£ thuy·∫øt h·ª£p l√Ω, s√°ng t·∫°o, c√≥ chi·ªÅu s√¢u.
        - Kh√¥ng gi·∫£i th√≠ch l·∫°i y√™u c·∫ßu, kh√¥ng th√™m l·ªùi m·ªü ƒë·∫ßu ho·∫∑c k·∫øt lu·∫≠n ngo√†i ph√¢n t√≠ch ch√≠nh.
        """

        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch d·ªØ li·ªáu t·ª± doanh...'})}\n\n"

        # B∆∞·ªõc 3: G·ªçi m√¥ h√¨nh Generative AI
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            async for chunk in generate_with_heartbeat(model, prompt, section_name="proprietary_trading_analysis"):
                yield chunk
        except Exception:
            yield f"data: {json.dumps({'type': 'error', 'section': 'proprietary_trading_analysis', 'message': 'L·ªói trong qu√° tr√¨nh ph√¢n t√≠ch'})}\n\n"

        yield f"data: {json.dumps({'type': 'section_end', 'section': 'proprietary_trading_analysis'})}\n\n"
        yield f"data: {json.dumps({'type': 'complete', 'message': 'Ph√¢n t√≠ch giao d·ªãch t·ª± doanh ho√†n t·∫•t!', 'progress': 100})}\n\n"
        
    except Exception:
        yield f"data: {json.dumps({'type': 'error', 'message': f'L·ªói h·ªá th·ªëng trong ph√¢n t√≠ch giao d·ªãch t·ª± doanh'})}\n\n"

async def get_foreign_trading_analysis_streaming(ticker: str):
    """
    Foreign trading analysis phase separated from get_insights_streaming.
    Returns a generator that yields Server-Sent Events formatted data.
    """
    
    
    try:
        # Phase 4: Foreign Trading Analysis
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch giao d·ªãch kh·ªëi ngo·∫°i...', 'progress': 10})}\n\n"
        yield f"data: {json.dumps({'type': 'section_start', 'section': 'foreign_trading_analysis', 'title': 'Ph√¢n T√≠ch Giao D·ªãch Kh·ªëi Ngo·∫°i'})}\n\n"
        # B∆∞·ªõc 1: L·∫•y d·ªØ li·ªáu kh·ªõp l·ªánh
        data = get_foreign_trading_data(symbol=ticker, start_date=None, end_date=None, page_index=1, page_size=14)
        df = pd.DataFrame(data)

        schema = {
            "Ngay": "Ng√†y giao d·ªãch",
            "KLGDRong": "Kh·ªëi l∆∞·ª£ng giao d·ªãch r√≤ng (mua tr·ª´ b√°n)",
            "GTDGRong": "Gi√° tr·ªã giao d·ªãch r√≤ng (t·ª∑ ƒë·ªìng, mua tr·ª´ b√°n)",
            "ThayDoi": "Bi·∫øn ƒë·ªông gi√° c·ªï phi·∫øu trong ng√†y (%)",
            "KLMua": "T·ªïng kh·ªëi l∆∞·ª£ng mua c·ªßa kh·ªëi ngo·∫°i",
            "GtMua": "T·ªïng gi√° tr·ªã mua c·ªßa kh·ªëi ngo·∫°i (t·ª∑ ƒë·ªìng)",
            "KLBan": "T·ªïng kh·ªëi l∆∞·ª£ng b√°n c·ªßa kh·ªëi ngo·∫°i",
            "GtBan": "T·ªïng gi√° tr·ªã b√°n c·ªßa kh·ªëi ngo·∫°i (t·ª∑ ƒë·ªìng)",
            "RoomConLai": "T·ª∑ l·ªá room ngo·∫°i c√≤n l·∫°i c√≥ th·ªÉ mua (%)",
            "DangSoHuu": "T·ª∑ l·ªá s·ªü h·ªØu hi·ªán t·∫°i c·ªßa kh·ªëi ngo·∫°i (%)"
            }
        
        df_json = df.to_json(orient="records", force_ascii=False)
        df = json.dumps({
            "schema": schema,
            "records": json.loads(df_json)
        }, indent=2, ensure_ascii=False)

        yield f"data: {json.dumps({'type': 'status', 'message': 'D·ªØ li·ªáu kh·ªõp l·ªánh ƒë√£ s·∫µn s√†ng...', 'progress': 50})}\n\n"

        # B∆∞·ªõc 2: T·∫°o prompt cho ph√¢n t√≠ch
        prompt = f"""
        B·∫°n l√† chuy√™n gia ph√¢n t√≠ch t√†i ch√≠nh chuy√™n nghi·ªáp. 
        H√£y ƒë√°nh gi√° chi ti·∫øt v√† ch√≠nh x√°c m√£ c·ªï phi·∫øu d·ª±a tr√™n d·ªØ li·ªáu giao d·ªãch kh·ªëi ngo·∫°i qu·ªëc d∆∞·ªõi ƒë√¢y.
        ƒê∆∞a ra c√°c nh·∫≠n ƒë·ªãnh chuy√™n m√¥n, gi·∫£ thuy·∫øt h·ª£p l√Ω c√≥ c∆° s·ªü.
        D·ªØ li·ªáu giao d·ªãch kh·ªëi ngo·∫°i qu·ªëc:
        {df}

        Y√™u c·∫ßu:
        - Tr·∫£ l·ªùi c·ª±c k√¨ KH√ÅCH QUAN mang t√≠nh chuy√™n m√¥n cao.
        - ƒê·ªçc hi·ªÉu s·ªë li·ªáu ƒë√£ cung c·∫•p th·∫≠t chuy√™n s√¢u.
        - Ph√¢n t√≠ch h√†nh vi giao d·ªãch c·ªßa kh·ªëi ngo·∫°i.
        - ƒê√°nh gi√° xu h∆∞·ªõng ni·ªÅm tin v√† t√°c ƒë·ªông t·ªõi gi√° c·ªï phi·∫øu.
        - ƒê∆∞a ra gi·∫£ thuy·∫øt h·ª£p l√Ω, s√°ng t·∫°o, c√≥ chi·ªÅu s√¢u.
        - Kh√¥ng gi·∫£i th√≠ch l·∫°i y√™u c·∫ßu, kh√¥ng th√™m l·ªùi m·ªü ƒë·∫ßu ho·∫∑c k·∫øt lu·∫≠n ngo√†i ph√¢n t√≠ch ch√≠nh.
        """

        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch d·ªØ li·ªáu kh·ªëi ngo·∫°i...'})}\n\n"

        # B∆∞·ªõc 3: G·ªçi m√¥ h√¨nh Generative AI
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            async for chunk in generate_with_heartbeat(model, prompt, section_name="foreign_trading_analysis"):
                yield chunk
        except Exception:
            yield f"data: {json.dumps({'type': 'error', 'section': 'foreign_trading_analysis', 'message': 'L·ªói trong qu√° tr√¨nh ph√¢n t√≠ch'})}\n\n"

        yield f"data: {json.dumps({'type': 'section_end', 'section': 'foreign_trading_analysis'})}\n\n"
        yield f"data: {json.dumps({'type': 'complete', 'message': 'Ph√¢n t√≠ch giao d·ªãch kh·ªëi ngo·∫°i ho√†n t·∫•t!', 'progress': 100})}\n\n"
        
    except Exception:
        yield f"data: {json.dumps({'type': 'error', 'message': f'L·ªói h·ªá th·ªëng trong ph√¢n t√≠ch giao d·ªãch kh·ªëi ngo·∫°i'})}\n\n"

async def get_shareholder_trading_analysis_streaming(ticker: str):
    """
    Shareholder trading analysis phase separated from get_insights_streaming.
    Returns a generator that yields Server-Sent Events formatted data.
    """
    try:
        # Phase 5: Shareholder Trading Analysis
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch giao d·ªãch c·ªï ƒë√¥ng...', 'progress': 10})}\n\n"
        yield f"data: {json.dumps({'type': 'section_start', 'section': 'shareholder_trading_analysis', 'title': 'Ph√¢n T√≠ch Giao D·ªãch C·ªï ƒê√¥ng N·ªôi B·ªô'})}\n\n"
        # B∆∞·ªõc 1: L·∫•y d·ªØ li·ªáu kh·ªõp l·ªánh
        data = get_shareholder_data(symbol=ticker, start_date=None, end_date=None, page_index=1, page_size=14)
        df = pd.DataFrame(data)
        df.drop(columns=['ShareHolderCode', 'HolderID'], inplace=True)

        schema = {
            "Stock": "M√£ c·ªï phi·∫øu",
            "TransactionMan": "Ng∆∞·ªùi th·ª±c hi·ªán giao d·ªãch (c·ªï ƒë√¥ng ho·∫∑c t·ªï ch·ª©c)",
            "TransactionManPosition": "Ch·ª©c v·ª• c·ªßa ng∆∞·ªùi giao d·ªãch trong c√¥ng ty",
            "RelatedMan": "Ng∆∞·ªùi ho·∫∑c t·ªï ch·ª©c c√≥ li√™n quan ƒë·∫øn ng∆∞·ªùi giao d·ªãch",
            "RelatedManPosition": "Ch·ª©c v·ª• c·ªßa ng∆∞·ªùi li√™n quan (n·∫øu c√≥)",
            "VolumeBeforeTransaction": "S·ªë l∆∞·ª£ng c·ªï phi·∫øu n·∫Øm gi·ªØ tr∆∞·ªõc giao d·ªãch",
            "PlanBuyVolume": "S·ªë l∆∞·ª£ng c·ªï phi·∫øu d·ª± ki·∫øn mua",
            "PlanSellVolume": "S·ªë l∆∞·ª£ng c·ªï phi·∫øu d·ª± ki·∫øn b√°n",
            "PlanBeginDate": "Ng√†y b·∫Øt ƒë·∫ßu k·∫ø ho·∫°ch giao d·ªãch",
            "PlanEndDate": "Ng√†y k·∫øt th√∫c k·∫ø ho·∫°ch giao d·ªãch",
            "RealBuyVolume": "S·ªë l∆∞·ª£ng c·ªï phi·∫øu th·ª±c t·∫ø ƒë√£ mua",
            "RealSellVolume": "S·ªë l∆∞·ª£ng c·ªï phi·∫øu th·ª±c t·∫ø ƒë√£ b√°n",
            "RealEndDate": "Ng√†y ho√†n t·∫•t giao d·ªãch th·ª±c t·∫ø",
            "PublishedDate": "Ng√†y c√¥ng b·ªë th√¥ng tin giao d·ªãch",
            "VolumeAfterTransaction": "S·ªë l∆∞·ª£ng c·ªï phi·∫øu c√≤n l·∫°i sau giao d·ªãch",
            "TransactionNote": "Ghi ch√∫ ho·∫∑c m·ª•c ƒë√≠ch giao d·ªãch (n·∫øu c√≥)",
            "TyLeSoHuu": "T·ª∑ l·ªá s·ªü h·ªØu c·ªï ph·∫ßn sau giao d·ªãch (%)",
            "OrderDate": "Ng√†y ƒë·∫∑t l·ªánh giao d·ªãch"
            }
        
        df_json = df.to_json(orient="records", force_ascii=False)
        df = json.dumps({
            "schema": schema,
            "records": json.loads(df_json)
        }, indent=2, ensure_ascii=False)

        yield f"data: {json.dumps({'type': 'status', 'message': 'D·ªØ li·ªáu kh·ªõp l·ªánh ƒë√£ s·∫µn s√†ng...', 'progress': 50})}\n\n"

        # B∆∞·ªõc 2: T·∫°o prompt cho ph√¢n t√≠ch
        prompt = f"""
        B·∫°n l√† chuy√™n gia ph√¢n t√≠ch t√†i ch√≠nh chuy√™n nghi·ªáp. 
        H√£y ƒë√°nh gi√° chi ti·∫øt v√† ch√≠nh x√°c m√£ c·ªï phi·∫øu d·ª±a tr√™n d·ªØ li·ªáu giao d·ªãch c·ªï ƒë√¥ng n·ªôi b·ªô d∆∞·ªõi ƒë√¢y.
        ƒê∆∞a ra c√°c nh·∫≠n ƒë·ªãnh chuy√™n m√¥n, gi·∫£ thuy·∫øt h·ª£p l√Ω c√≥ c∆° s·ªü.
        D·ªØ li·ªáu giao d·ªãch gi·ªØa c·ªï ƒë√¥ng c·ªßa c√¥ng ty:
        {df}

        Y√™u c·∫ßu:
        - Tr·∫£ l·ªùi c·ª±c k√¨ KH√ÅCH QUAN mang t√≠nh chuy√™n m√¥n cao.
        - ƒê·ªçc hi·ªÉu s·ªë li·ªáu ƒë√£ cung c·∫•p th·∫≠t chuy√™n s√¢u.
        - Ph√¢n t√≠ch h√†nh vi giao d·ªãch c·ªßa c·ªï ƒë√¥ng n·ªôi b·ªô.
        - ƒê√°nh gi√° xu h∆∞·ªõng ni·ªÅm tin v√† t√°c ƒë·ªông t·ªõi gi√° c·ªï phi·∫øu.
        - ƒê∆∞a ra gi·∫£ thuy·∫øt h·ª£p l√Ω, s√°ng t·∫°o, c√≥ chi·ªÅu s√¢u.
        - Kh√¥ng gi·∫£i th√≠ch l·∫°i y√™u c·∫ßu, kh√¥ng th√™m l·ªùi m·ªü ƒë·∫ßu ho·∫∑c k·∫øt lu·∫≠n ngo√†i ph√¢n t√≠ch ch√≠nh.
        """

        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang ph√¢n t√≠ch d·ªØ li·ªáu giao d·ªãch c·ªï ƒë√¥ng...'})}\n\n"

        # B∆∞·ªõc 3: G·ªçi m√¥ h√¨nh Generative AI
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            async for chunk in generate_with_heartbeat(model, prompt, section_name="shareholder_trading_analysis"):
                yield chunk
        except Exception:
            yield f"data: {json.dumps({'type': 'error', 'section': 'shareholder_trading_analysis', 'message': 'L·ªói trong qu√° tr√¨nh ph√¢n t√≠ch'})}\n\n"

        yield f"data: {json.dumps({'type': 'section_end', 'section': 'shareholder_trading_analysis'})}\n\n"
        yield f"data: {json.dumps({'type': 'complete', 'message': 'Ph√¢n t√≠ch giao d·ªãch c·ªï ƒë√¥ng ho√†n t·∫•t!', 'progress': 100})}\n\n"
        
    except Exception:
        yield f"data: {json.dumps({'type': 'error', 'message': f'L·ªói h·ªá th·ªëng trong ph√¢n t√≠ch giao d·ªãch c·ªï ƒë√¥ng'})}\n\n"

async def fetch_news_streaming(
    symbol: str,
    asset_type: str = 'stock',
    look_back_days: int = 30,
    pages: int = 2,
    max_results: int = 50,
    news_sources: List[str] = ['google']
):
    """
    Streaming version of news fetching that yields chunks in real-time.
    Returns a generator that yields Server-Sent Events formatted data.
    """
    import json
    import asyncio
    import time
    from datetime import datetime, timedelta
    
    symbol = symbol.upper().strip()
    
    async def send_heartbeat_during_operation(operation_name: str, progress: int = 0):
        """Send heartbeat during long operations"""
        yield f"data: {json.dumps({'type': 'status', 'message': f'ü§ñ ƒêang {operation_name}...', 'progress': progress, 'heartbeat': True})}\n\n"
        await asyncio.sleep(0.1)
    
    try:
        # Initialize news aggregation
        aggregated_news = []
        news_stats = {
            'total_articles': 0,
            'sources_used': [],
            'date_range': {
                'from': (datetime.now() - timedelta(days=look_back_days)).strftime('%Y-%m-%d'),
                'to': datetime.now().strftime('%Y-%m-%d')
            },
            'processing_time': 0
        }
        
        start_time = datetime.now()
        
        # Yield initial status
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang kh·ªüi t·∫°o t√¨m ki·∫øm tin t·ª©c...', 'progress': 5})}\n\n"
        
        # Yield news collection start
        yield f"data: {json.dumps({'type': 'section_start', 'section': 'news_collection', 'title': f'Thu Th·∫≠p Tin T·ª©c - {symbol}'})}\n\n"
        
        # (universal source)
        if 'google' in news_sources:
            try:
                yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang t√¨m ki·∫øm tr√™n Google News...', 'progress': 20})}\n\n"
                message = f'üîç **ƒêang t√¨m ki·∫øm tin t·ª©c v·ªÅ {symbol} tr√™n Google News...**\n\n'
                yield f"data: {json.dumps({'type': 'content', 'section': 'news_collection', 'text': message})}\n\n"

                # Create search query based on stock type
                if asset_type == 'stock':
                    # Remove .VN suffix for Vietnamese stocks
                    clean_symbol = symbol.replace('.VN', '')
                    search_query = f"tin t·ª©c c·ªï phi·∫øu {clean_symbol} OR c√¥ng ty {clean_symbol} OR m√£ {clean_symbol}"
                elif asset_type == 'crypto':
                    search_query = f"Important news for crypto currencies ticket {symbol}"

                # Add heartbeat before long operation
                async for heartbeat in send_heartbeat_during_operation("T√¨m ki·∫øm tin t·ª©c", 25):
                    yield heartbeat

                google_news = fetch_google_news(
                    search_query,
                    datetime.now().strftime('%Y-%m-%d'),
                    look_back_days
                )
                
                if google_news:
                    yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang x·ª≠ l√Ω k·∫øt qu·∫£...', 'progress': 40})}\n\n"
                    
                    # Parse format with heartbeat
                    async for heartbeat in send_heartbeat_during_operation("Ph√¢n t√≠ch c√∫ ph√°p tin t·ª©c", 42):
                        yield heartbeat
                    
                    from app_fastapi import parse_google_news_format
                    google_articles = parse_google_news_format(google_news, 'Google News')
                    
                    message = f'‚úÖ **T√¨m th·∫•y {len(google_articles)} b√†i vi·∫øt t·ª´ Google News**\n\n'
                    yield f"data: {json.dumps({'type': 'content', 'section': 'news_collection', 'text': message})}\n\n"

                    # Stream individual articles with heartbeat
                    total_articles = len(google_articles[:max_results//2])
                    for i, article in enumerate(google_articles[:max_results//2]):
                        aggregated_news.append(article)
                        
                        # Stream article info
                        article_text = f"üì∞ **{article.get('title', 'No title')}**\\n"
                        article_text += f"üìÖ {article.get('date', 'No date')} | üîó {article.get('source', 'Unknown source')}\\n"
                        article_text += f"üìä ƒêi·ªÉm li√™n quan: {article.get('relevance_score', 0):.1f}\\n\\n"
                        
                        yield f"data: {json.dumps({'type': 'content', 'section': 'news_collection', 'text': article_text})}\n\n"
                        
                        # Update progress
                        progress = min(40 + (i / total_articles) * 30, 70)
                        yield f"data: {json.dumps({'type': 'status', 'message': f'ƒê√£ x·ª≠ l√Ω {i+1}/{total_articles} b√†i vi·∫øt...', 'progress': progress})}\n\n"
                        
                        # Small delay for streaming effect with async support
                        await asyncio.sleep(0.1)
                    
                    news_stats['sources_used'].append('google')
                    
                else:
                    message = '‚ö†Ô∏è **Kh√¥ng t√¨m th·∫•y tin t·ª©c t·ª´ Google News**\\n\\n'
                    yield f"data: {json.dumps({'type': 'content', 'section': 'news_collection', 'text': message})}\n\n"
                    
            except Exception as e:
                error_msg = f"‚ùå **L·ªói khi t√¨m ki·∫øm:** {str(e)}\\n\\n"
                yield f"data: {json.dumps({'type': 'content', 'section': 'news_collection', 'text': error_msg})}\n\n"
        
        # Process and enhance news with heartbeat
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang x·ª≠ l√Ω v√† ph√¢n t√≠ch tin t·ª©c...', 'progress': 75})}\n\n"
        
        # Remove duplicates based on title similarity with heartbeat
        if aggregated_news:
            message = 'üîÑ **ƒêang lo·∫°i b·ªè tin t·ª©c tr√πng l·∫∑p...**\\n\\n'
            yield f"data: {json.dumps({'type': 'content', 'section': 'news_collection', 'text': message})}\n\n"
            
            # Add heartbeat for duplicate removal
            async for heartbeat in send_heartbeat_during_operation("Lo·∫°i b·ªè tin t·ª©c tr√πng l·∫∑p", 77):
                yield heartbeat
            
            from app_fastapi import remove_duplicate_news
            original_count = len(aggregated_news)
            aggregated_news = remove_duplicate_news(aggregated_news)
            removed_count = original_count - len(aggregated_news)
            
            if removed_count > 0:
                message = f'‚úÖ **ƒê√£ lo·∫°i b·ªè {removed_count} tin t·ª©c tr√πng l·∫∑p**\\n\\n'
                yield f"data: {json.dumps({'type': 'content', 'section': 'news_collection', 'text': message})}\n\n"
        
        # Add sentiment analysis with heartbeat
        if aggregated_news:
            message = 'üß† **ƒêang ph√¢n t√≠ch c·∫£m x√∫c tin t·ª©c...**\\n\\n'
            yield f"data: {json.dumps({'type': 'content', 'section': 'news_collection', 'text': message})}\n\n"
            
            # Add heartbeat for sentiment analysis
            async for heartbeat in send_heartbeat_during_operation("Ph√¢n t√≠ch c·∫£m x√∫c tin t·ª©c", 80):
                yield heartbeat
            
            from app_fastapi import enhance_news_with_sentiment
            aggregated_news = enhance_news_with_sentiment(aggregated_news)
            
            # Show sentiment summary
            positive_count = sum(1 for news in aggregated_news if news.get('sentiment') == 'positive')
            negative_count = sum(1 for news in aggregated_news if news.get('sentiment') == 'negative')
            neutral_count = len(aggregated_news) - positive_count - negative_count
            
            sentiment_text = f"üìä **Ph√¢n t√≠ch c·∫£m x√∫c:**\\n"
            sentiment_text += f"üìà T√≠ch c·ª±c: {positive_count} b√†i\\n"
            sentiment_text += f"üìâ Ti√™u c·ª±c: {negative_count} b√†i\\n"
            sentiment_text += f"üìä Trung t√≠nh: {neutral_count} b√†i\\n\\n"
            
            yield f"data: {json.dumps({'type': 'content', 'section': 'news_collection', 'text': sentiment_text})}\n\n"
        
        # Sort by relevance score and date with heartbeat
        if aggregated_news:
            async for heartbeat in send_heartbeat_during_operation("S·∫Øp x·∫øp tin t·ª©c theo ƒë·ªô li√™n quan", 85):
                yield heartbeat
            aggregated_news.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        
        # Limit results
        aggregated_news = aggregated_news[:max_results]
        
        # Update statistics
        news_stats['total_articles'] = len(aggregated_news)
        news_stats['processing_time'] = (datetime.now() - start_time).total_seconds()
        
        # End news collection section
        yield f"data: {json.dumps({'type': 'section_end', 'section': 'news_collection'})}\n\n"
        
        # Start news results section
        yield f"data: {json.dumps({'type': 'status', 'message': 'ƒêang chu·∫©n b·ªã k·∫øt qu·∫£...', 'progress': 90})}\n\n"
        yield f"data: {json.dumps({'type': 'section_start', 'section': 'news_results', 'title': f'K·∫øt Qu·∫£ Tin T·ª©c - {len(aggregated_news)} b√†i vi·∫øt'})}\n\n"
        
        # Stream final results with heartbeat for large datasets
        if aggregated_news:
            total_news = len(aggregated_news)
            for i, news in enumerate(aggregated_news):
                news_data = {
                    'id': news.get('id', ''),
                    'title': news.get('title', 'No title'),
                    'content': news.get('content', news.get('snippet', news.get('summary', news.get('description', 'No content available')))),
                    'sentiment': news.get('sentiment', 'neutral'),
                    'score': news.get('sentiment_score', news.get('relevance_score', 0)),
                    'publishedAt': news.get('published_at', news.get('date', datetime.now().isoformat())),
                    'source': news.get('source', 'Unknown'),
                    'url': news.get('url', news.get('link', '#'))  # Add URL field
                }
                
                yield f"data: {json.dumps({'type': 'news_item', 'section': 'news_results', 'data': news_data})}\n\n"
                
                # Add heartbeat every 10 items for large datasets
                if total_news > 20 and (i + 1) % 10 == 0:
                    progress = 90 + ((i + 1) / total_news) * 8
                    async for heartbeat in send_heartbeat_during_operation(f"ƒêang truy·ªÅn tin t·ª©c ({i+1}/{total_news})", int(progress)):
                        yield heartbeat
                
                # Small delay for streaming effect
                await asyncio.sleep(0.05)
        else:
            message = '‚ö†Ô∏è **Kh√¥ng t√¨m th·∫•y tin t·ª©c n√†o ph√π h·ª£p.**\\n\\n'
            yield f"data: {json.dumps({'type': 'content', 'section': 'news_results', 'text': message})}\n\n"
        
        # End news results section
        yield f"data: {json.dumps({'type': 'section_end', 'section': 'news_results'})}\n\n"
        
        # Final response data with heartbeat
        async for heartbeat in send_heartbeat_during_operation("Chu·∫©n b·ªã d·ªØ li·ªáu cu·ªëi c√πng", 98):
            yield heartbeat
            
        final_response = {
            'status': 'success',
            'data': aggregated_news,
            'symbol': symbol,
            'metadata': {
                'symbol_type': 'vietnamese' if not any(char in symbol for char in ['.', ':']) or symbol.endswith('.VN') else 'global',
                'search_parameters': {
                    'symbol': symbol,
                    'pages': pages,
                    'look_back_days': look_back_days,
                    'news_sources': news_sources,
                    'max_results': max_results
                },
                'statistics': news_stats
            }
        }
        
        # Send final data
        yield f"data: {json.dumps({'type': 'final_data', 'data': final_response})}\n\n"
        
        # Completion
        yield f"data: {json.dumps({'type': 'complete', 'message': f'Ho√†n t·∫•t! T√¨m th·∫•y {len(aggregated_news)} tin t·ª©c v·ªÅ {symbol}', 'progress': 100})}\n\n"
        
    except Exception as e:
        yield f"data: {json.dumps({'type': 'error', 'message': f'L·ªói h·ªá th·ªëng: {str(e)}'})}\n\n"

# if __name__ == "__main__":
#     print(get_shareholder_transaction_analysis_streaming("VIC"))